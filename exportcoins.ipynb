{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from requests import get as curl\n",
    "import ujson as json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "import os\n",
    "from numba import jit\n",
    "\n",
    "## Plotting imports\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2 as postgre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coinsdb = postgre.connect(\"MY LOCAL DB CREDENTIALS\")\n",
    "cursor = coinsdb.cursor()\n",
    "coinsdb.rollback() # Clear any previous errors.\n",
    "# cursor.execute(f\"SELECT symbols FROM symbols;\")\n",
    "# symbols = [x[0].lower() for x in cursor.fetchall() ]\n",
    "def run(s):\n",
    "    cursor.execute(s)\n",
    "    print( cursor.fetchall() )\n",
    "\n",
    "# List of strings with the names of all data tables in DB\n",
    "cursor.execute(f\"SELECT tables FROM tables;\")\n",
    "tables = [x[0].lower() for x in cursor.fetchall() ]\n",
    "\n",
    "numDays = 7 # last 7 days\n",
    "numrows = numDays * 24 # times hourly measurements \n",
    "numrows += 1 # add one more as the day0 measurement for percent changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We clean timestamps by rounding them to nearest hour, so that charts look nicer (every hour sharp) \n",
    "def clean_index(index):\n",
    "    newIdx = []\n",
    "    for idx in index:\n",
    "        newIdx.append(pd.Timestamp(str( idx )[:13] + \":00:00\"))\n",
    "    return pd.DatetimeIndex(newIdx)\n",
    "\n",
    "data = dict()  # dict of panda Dataframes, each entry is one table from coins.db\n",
    "for table in tables:\n",
    "    # QUERY all columns from this table\n",
    "    cursor.execute(f\"SELECT column_name FROM information_schema.columns WHERE table_name = '{table}';\")\n",
    "    colNames = [x[0] for x in cursor.fetchall() ]#eliminate timestamp from column names\n",
    "    \n",
    "    # Get most recent #numrows of data \n",
    "    cursor.execute(f\"SELECT * FROM (SELECT * FROM {table} ORDER BY timestamp DESC LIMIT {numrows}) as rows ORDER BY timestamp ASC;\")\n",
    "    cols = cursor.fetchall()\n",
    "    \n",
    "    # Process to build dataframe with clean index. Using pandas.read_sql would read the entire db which is not feasible.\n",
    "    # Another reason is we found a bug with read_sql where some tables would not load completely at sometimes.\n",
    "    vals = dict()\n",
    "    for i, colName in enumerate(colNames):\n",
    "        if i == 0:\n",
    "            idx = [col[0] for col in cols]\n",
    "        else:\n",
    "            vals[colName.upper()] = [col[i] for col in cols]\n",
    "    \n",
    "    data[table] = pd.DataFrame.from_dict(vals)\n",
    "    data[table].index = clean_index( idx )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies boolean function to cols or rows of df and returns those which return true.\n",
    "def filterDF(df: pd.DataFrame, f, axis=0):\n",
    "    res = []\n",
    "    if axis in [0, 'columns']:\n",
    "        for col in df:\n",
    "            if f(col):\n",
    "                res.append(col.name)\n",
    "        return df.loc[: , res]\n",
    "    elif axis in [1,'rows','index']:\n",
    "        for row in df.index:\n",
    "            if df.loc[row,:].apply(f):\n",
    "                res.append(row.name)\n",
    "        return df.loc[res , :]\n",
    "\n",
    "# Return elements of df where f(elem) == True in a list\n",
    "def filter_elements(df, f):\n",
    "    res = []\n",
    "    for row in df.index:\n",
    "        for col in df.columns:\n",
    "            val = df.loc[row, col]\n",
    "            if f( val ):\n",
    "                res.append([ row , col , val ])\n",
    "    return res\n",
    "\n",
    "# Retrieves the last n entries to analyze and cleans the entries of missing or 0-values and duplicates\n",
    "# This cleaning is due to Binance's API failures. \n",
    "# We later find the common columns free of failures for all tables\n",
    "def last(table, numrows):\n",
    "    # Drop the date and time to leave only data points as entries\n",
    "    df = data[table].tail(numrows).dropna(axis='columns')\n",
    "    goodcols = df.apply(lambda col: (len(col.unique()) > len(col)/2 ) & ((col != 0).all()) )\n",
    "    return df.loc[:, goodcols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prices_raw = last('weightedavgprice', numrows)\n",
    "volumes_raw = last('quotevolume', numrows)\n",
    "\n",
    "startDate = prices_raw.index[0].date()\n",
    "endDate = prices_raw.index[-1].date() # Most recent day in data. Should be today.\n",
    "\n",
    "# Here we store the daily USD trading value for non-USD quote coins so each volume and price are shown relative to USD. \n",
    "# As of this writing, these are the only trading coins in Binance: ['BTC', 'ETH','BNB','TUSD','PAX','USDT', 'USDC']\n",
    "# For simplicity, we don't convert USDT and TUSD pairs because:\n",
    "#  1) a USDT-USDC or TUSD-USDC pair does not exist yet to facilitate conversion\n",
    "#  2) since they are all USD-based stablecoins they should not have any difference.\n",
    "endings = dict(BTC=prices_raw['BTCUSDC'], ETH=prices_raw['ETHUSDC'], XRP=prices_raw['XRPUSDC'],\n",
    "               BNB=prices_raw['BNBUSDC'], PAX=prices_raw['PAXUSDT'], \n",
    "               SDT=1, USD=1, SDC=1)\n",
    "\n",
    "# Function to convert all quoteVolumes to BTC-volumes for comparison. apply to volumesPlus1\n",
    "def convert_USD(col):\n",
    "    if col.name[:4] == 'usdc':\n",
    "        return col\n",
    "    else:\n",
    "        return col * endings[col.name[-3:]]\n",
    "\n",
    "# We have to find common columns because the source data is not clean nor correct among all tables.\n",
    "common_cols = {x for x in prices_raw.columns if x[:4] != 'usdc'}.intersection( set(volumes_raw.columns) )\n",
    "\n",
    "# Convert all values to USDC so we can standardize their comparison around their USD value.\n",
    "pricesPlus1 = prices_raw.loc[:, common_cols].apply( convert_USD )\n",
    "volumesPlus1 = volumes_raw.loc[:, common_cols].apply( convert_USD )\n",
    "\n",
    "# Make list of top Volumes (standardized in USD) to filter out coins with high price fluctuations but low volume. \n",
    "topVols = volumesPlus1.loc[:, volumesPlus1.mean() >= volumesPlus1.mean().median() ].columns\n",
    "excluded_pairs = list( set(data['quotevolume'].columns) - set(topVols) )\n",
    "\n",
    "# baseVolume = volumesPlus1.iloc[0][topVols]\n",
    "# basePrice = pricesPlus1.iloc[0][topVols]\n",
    "\n",
    "# These are ready to graph df's\n",
    "prices = pricesPlus1.iloc[1:][topVols]\n",
    "volumes = volumesPlus1.iloc[1:][topVols]\n",
    "\n",
    "correlationPrices = prices.corr()\n",
    "correlationVolumes = volumes.corr()\n",
    "correlationValues = volumes.corrwith(prices).sort_values(ascending=False)\n",
    "\n",
    "# These df's contain daily percent change from day1 to day7. \n",
    "# With day1 being the change from day0 (basePrice/Vol). \n",
    "volumePercents = volumesPlus1[topVols].pct_change().iloc[1:] * 100\n",
    "pricePercents = pricesPlus1[topVols].pct_change().iloc[1:] * 100\n",
    "\n",
    "correlationVolumePercents = volumePercents.corr()\n",
    "correlationPricePercents = pricePercents.corr()\n",
    "correlationPercents = volumePercents.corrwith(pricePercents).sort_values(ascending=False)\n",
    "\n",
    "# Log returns is a measure used in finance similar to percent change. \n",
    "def log_returns(df):\n",
    "    return df.applymap(np.log).diff()[topVols].iloc[1:]\n",
    "\n",
    "volumesLogReturn = log_returns(volumesPlus1)\n",
    "pricesLogReturn = log_returns(pricesPlus1)\n",
    "\n",
    "correlationVolumesLogReturn = volumesLogReturn.corr()\n",
    "correlationPricesLogReturn = pricesLogReturn.corr()\n",
    "correlationLogReturns = pricesLogReturn.corrwith(volumesLogReturn).sort_values(ascending=False)\n",
    "\n",
    "# This finds the total percent change from day0 to day7 and result is in percent to display in graph directly.\n",
    "def total_percent_change(df: pd.DataFrame):\n",
    "    return df.applymap(lambda x: x/100 + 1).prod().apply(lambda x: (x - 1) * 100)\n",
    "\n",
    "# Series showing the total change in price from day0 to today\n",
    "totalPrice = total_percent_change(pricePercents)\n",
    "totalVolume = total_percent_change(volumePercents)\n",
    "\n",
    "def cumulative_percent_change(df: pd.DataFrame):\n",
    "    return df.applymap(lambda x: x/100 + 1).cumprod().apply(lambda x: (x - 1) * 100)\n",
    "\n",
    "# Cumulative hourly percent change if we had invested numDays ago:\n",
    "cumuPrice = cumulative_percent_change(pricePercents)\n",
    "cumuVolume = cumulative_percent_change(volumePercents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done exporting files to /static/json\n",
      "Local Time: 2019-01-11 07:35:48.917311\n"
     ]
    }
   ],
   "source": [
    "# This class right here only exists because pandas dict(df) != df.to_list() \n",
    "# so to avoid different type handling in export_json, we made this.    \n",
    "class Corrs(list):\n",
    "    def to_dict(self):\n",
    "        return dict(self)\n",
    "\n",
    "# Prepares df.corr() as list for exporting\n",
    "def list_corr(df):\n",
    "    corrs = Corrs() # stores our result as a list of tuples for sorting at the end\n",
    "    for i in range(1, df.shape[0]):\n",
    "        for j in range(i):\n",
    "            row = df.index[i]\n",
    "            col = df.columns[j]\n",
    "            corrs.append( (f\"{row}-{col}\", df.iloc[i, j]) )\n",
    "            \n",
    "    return Corrs( sorted(corrs, key=lambda x: x[1], reverse=True) )\n",
    "\n",
    "# Export data to json files.\n",
    "folder = 'static/json'\n",
    "def export_json(d, filename, graphType):\n",
    "    with open(f\"{folder}/{filename}.json\", 'w') as f:\n",
    "        json.dump({'data': d.to_dict(), 'startDate': str(startDate), 'endDate': str(endDate), \n",
    "                   'numDays':numDays, 'excludedPairs': excluded_pairs, 'graphType': graphType} ,\n",
    "                  f)\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "export_json( cumuPrice , 'cumulative-price-percent', 'dygraph' )\n",
    "export_json( cumuVolume , 'cumulative-volume-percent', 'dygraph' )\n",
    "\n",
    "export_json( totalPrice , 'total-price-percent-change', 'hbar' )\n",
    "export_json( totalVolume , 'total-volume-percent-change', 'hbar' )\n",
    "\n",
    "export_json( prices , 'prices', 'dygraph' )\n",
    "export_json( volumes , 'volumes', 'dygraph' )\n",
    "\n",
    "export_json( pricePercents , 'percent-prices', 'dygraph' )\n",
    "export_json( volumePercents , 'percent-volumes', 'dygraph' )\n",
    "\n",
    "export_json( pricesLogReturn , 'log-return-prices', 'dygraph' )\n",
    "export_json( volumesLogReturn , 'log-return-volumes', 'dygraph' )\n",
    "\n",
    "export_json( list_corr( correlationVolumes ), 'correlation-volumes-pvp', 'hbar' )\n",
    "export_json( list_corr( correlationPrices ), 'correlation-prices-pvp', 'hbar' )\n",
    "\n",
    "export_json( list_corr( correlationVolumePercents ), 'correlation-volume-percents-pvp', 'hbar' )\n",
    "export_json( list_corr( correlationPricePercents ), 'correlation-price-percents-pvp', 'hbar' )\n",
    "\n",
    "export_json( list_corr(correlationVolumesLogReturn), 'correlation-volumes-log-return-pvp', 'hbar' )\n",
    "export_json( list_corr(correlationPricesLogReturn), 'correlation-prices-log-return-pvp', 'hbar' )\n",
    "\n",
    "export_json( correlationValues, 'correlation-volume-price', 'hbar' )\n",
    "export_json( correlationPercents, 'correlation-volume-price-percents', 'hbar' )\n",
    "export_json( correlationLogReturns, 'correlation-volume-price-log-returns', 'hbar' )\n",
    "\n",
    "\n",
    "print(f'Done exporting files to /static/json')\n",
    "print(f'Local Time: {datetime.now()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Algorithm for correlation of percent changes\n",
    "log returns, instead of percent changes for correlation:\n",
    "\n",
    "returns = pd.DataFrame(data={ col: np.log(prices[col]).diff() for col in prices}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coinlist = [\"NANO\",\"IOTA\",\"XMR\", \"EOS\",\"XRP\",\"NEO\",\"DASH\"]\n",
    "# # # coinlist = [\"XRP\"]\n",
    "# re = lambda symbol: f\"^{symbol}|{symbol}$\"\n",
    "# end = lambda symbol: f\"{symbol}$\"\n",
    "# regstr = reduce(lambda x,y: x + \"|\" + re(y), [''] + coinlist)[1:]\n",
    "# regstr"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# QUERY COLUMN NAMES FOR A TABLE\n",
    "# run(\"SELECT column_name FROM information_schema.columns WHERE table_name = 'quotevolume';\")\n",
    "\n",
    "# QUERY TABLE NAMES FOR THE ENTIRE DB\n",
    "# run(\"\"\"SELECT table_name\n",
    "#   FROM information_schema.tables\n",
    "#  WHERE table_schema='public'\n",
    "#    AND table_type='BASE TABLE';\"\"\")\n",
    "\n",
    "# cursor.execute(\"SELECT column_name FROM information_schema.columns WHERE table_name = 'quotevolume';\")\n",
    "# s = [x[0] for x in cursor.fetchall()]\n",
    "# \", \".join( s )\n",
    "\n",
    "# cursor.execute(f\"SELECT * FROM log \")\n",
    "# cursor.fetchall()\n",
    "\n",
    "# table = 'askPrice'\n",
    "# sql_cursor.execute( SQL(\"ALTER TABLE {} ADD COLUMN {} DOUBLE PRECISION;\").format(Identifier(table), Identifier(coin)) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
